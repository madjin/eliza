"use strict";(self.webpackChunkeliza_docs=self.webpackChunkeliza_docs||[]).push([[3822],{7495:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var t=i(4848),l=i(8453);const o={sidebar_position:2},a="Quickstart",s={id:"quickstart",title:"Quickstart",description:"Install Node.js",source:"@site/docs/quickstart.md",sourceDirName:".",slug:"/quickstart",permalink:"/eliza/docs/quickstart",draft:!1,unlisted:!1,editUrl:"https://github.com/ai16z/eliza/tree/main/docs/docs/quickstart.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Introduction",permalink:"/eliza/docs/intro"},next:{title:"Installation",permalink:"/eliza/docs/installation"}},r={},d=[{value:"Install Node.js",id:"install-nodejs",level:2},{value:"Using pnpm",id:"using-pnpm",level:2},{value:"Edit the .env file",id:"edit-the-env-file",level:2},{value:"Edit the character file",id:"edit-the-character-file",level:2},{value:"Run with Llama",id:"run-with-llama",level:3},{value:"Run with Grok",id:"run-with-grok",level:3},{value:"Run with OpenAI",id:"run-with-openai",level:3},{value:"Additional Requirements",id:"additional-requirements",level:2},{value:"CUDA Setup",id:"cuda-setup",level:2},{value:"Running locally",id:"running-locally",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"quickstart",children:"Quickstart"})}),"\n",(0,t.jsx)(n.h2,{id:"install-nodejs",children:"Install Node.js"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://docs.npmjs.com/downloading-and-installing-node-js-and-npm",children:"https://docs.npmjs.com/downloading-and-installing-node-js-and-npm"})}),"\n",(0,t.jsx)(n.h2,{id:"using-pnpm",children:"Using pnpm"}),"\n",(0,t.jsxs)(n.p,{children:["We use pnpm to manage our dependencies. It is faster and more efficient than npm, and it supports workspaces.\n",(0,t.jsx)(n.a,{href:"https://pnpm.io/installation",children:"https://pnpm.io/installation"})]}),"\n",(0,t.jsx)(n.h2,{id:"edit-the-env-file",children:"Edit the .env file"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Copy .env.example to .env and fill in the appropriate values"}),"\n",(0,t.jsx)(n.li,{children:"Edit the TWITTER environment variables to add your bot's username and password"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"edit-the-character-file",children:"Edit the character file"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Check out the file ",(0,t.jsx)(n.code,{children:"src/core/defaultCharacter.ts"})," - you can modify this"]}),"\n",(0,t.jsxs)(n.li,{children:["You can also load characters with the ",(0,t.jsx)(n.code,{children:'node --loader ts-node/esm src/index.ts --characters="path/to/your/character.json"'})," and run multiple bots at the same time."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"run-with-llama",children:"Run with Llama"}),"\n",(0,t.jsxs)(n.p,{children:["You can run Llama 70B or 405B models by setting the ",(0,t.jsx)(n.code,{children:"XAI_MODEL"})," environment variable to ",(0,t.jsx)(n.code,{children:"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"})," or ",(0,t.jsx)(n.code,{children:"meta-llama/Meta-Llama-3.1-405B-Instruct"})]}),"\n",(0,t.jsx)(n.h3,{id:"run-with-grok",children:"Run with Grok"}),"\n",(0,t.jsxs)(n.p,{children:["You can run Grok models by setting the ",(0,t.jsx)(n.code,{children:"XAI_MODEL"})," environment variable to ",(0,t.jsx)(n.code,{children:"grok-beta"})]}),"\n",(0,t.jsx)(n.h3,{id:"run-with-openai",children:"Run with OpenAI"}),"\n",(0,t.jsxs)(n.p,{children:["You can run OpenAI models by setting the ",(0,t.jsx)(n.code,{children:"XAI_MODEL"})," environment variable to ",(0,t.jsx)(n.code,{children:"gpt-4o-mini"})," or ",(0,t.jsx)(n.code,{children:"gpt-4o"})]}),"\n",(0,t.jsx)(n.h1,{id:"requires-node-20",children:"Requires Node 20+"}),"\n",(0,t.jsxs)(n.p,{children:["If you are getting strange issues when starting up, make sure you're using Node 20+. Some APIs are not compatible with previous versions. You can check your node version with ",(0,t.jsx)(n.code,{children:"node -v"}),". If you need to install a new version of node, we recommend using ",(0,t.jsx)(n.a,{href:"https://github.com/nvm-sh/nvm",children:"nvm"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"additional-requirements",children:"Additional Requirements"}),"\n",(0,t.jsx)(n.p,{children:"You may need to install Sharp. If you see an error when starting up, try installing it with the following command:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"pnpm install --include=optional sharp\n"})}),"\n",(0,t.jsx)(n.h1,{id:"environment-setup",children:"Environment Setup"}),"\n",(0,t.jsx)(n.p,{children:"You will need to add environment variables to your .env file to connect to various platforms:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"# Required environment variables\n# Start Discord\nDISCORD_APPLICATION_ID=\nDISCORD_API_TOKEN= # Bot token\n\n# Start Twitter\nTWITTER_USERNAME= # Account username\nTWITTER_PASSWORD= # Account password\nTWITTER_EMAIL= # Account email\nTWITTER_COOKIES= # Account cookies\n"})}),"\n",(0,t.jsx)(n.h1,{id:"local-setup",children:"Local Setup"}),"\n",(0,t.jsx)(n.h2,{id:"cuda-setup",children:"CUDA Setup"}),"\n",(0,t.jsx)(n.p,{children:"If you have an NVIDIA GPU, you can install CUDA to speed up local inference dramatically."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"pnpm install\nnpx --no node-llama-cpp source download --gpu cuda\n"})}),"\n",(0,t.jsx)(n.p,{children:"Make sure that you've installed the CUDA Toolkit, including cuDNN and cuBLAS."}),"\n",(0,t.jsx)(n.h2,{id:"running-locally",children:"Running locally"}),"\n",(0,t.jsxs)(n.p,{children:["Add XAI_MODEL and set it to one of the above options from ",(0,t.jsx)(n.a,{href:"#run-with-llama",children:"Run with\nLlama"})," - you can leave X_SERVER_URL and XAI_API_KEY blank, it\ndownloads the model from huggingface and queries it locally"]}),"\n",(0,t.jsx)(n.h1,{id:"cloud-setup-with-openai",children:"Cloud Setup (with OpenAI)"}),"\n",(0,t.jsx)(n.p,{children:"In addition to the environment variables above, you will need to add the following:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"# OpenAI handles the bulk of the work with chat, TTS, image recognition, etc.\nOPENAI_API_KEY=sk-* # OpenAI API key, starting with sk-\n\n# The agent can also ask Claude for help if you have an API key\nANTHROPIC_API_KEY=\n\n# For Elevenlabs voice generation on Discord voice\nELEVENLABS_XI_API_KEY= # API key from elevenlabs\n\n# ELEVENLABS SETTINGS\nELEVENLABS_MODEL_ID=eleven_multilingual_v2\nELEVENLABS_VOICE_ID=21m00Tcm4TlvDq8ikWAM\nELEVENLABS_VOICE_STABILITY=0.5\nELEVENLABS_VOICE_SIMILARITY_BOOST=0.9\nELEVENLABS_VOICE_STYLE=0.66\nELEVENLABS_VOICE_USE_SPEAKER_BOOST=false\nELEVENLABS_OPTIMIZE_STREAMING_LATENCY=4\nELEVENLABS_OUTPUT_FORMAT=pcm_16000\n"})}),"\n",(0,t.jsx)(n.h1,{id:"discord-bot",children:"Discord Bot"}),"\n",(0,t.jsxs)(n.p,{children:["For help with setting up your Discord Bot, check out here: ",(0,t.jsx)(n.a,{href:"https://discordjs.guide/preparations/setting-up-a-bot-application.html",children:"https://discordjs.guide/preparations/setting-up-a-bot-application.html"})]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>s});var t=i(6540);const l={},o=t.createContext(l);function a(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);